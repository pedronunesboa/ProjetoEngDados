# Projeto MarketPulse: Pipeline de Engenharia de Dados

Este repositÃ³rio contÃ©m um projeto completo de pipeline de engenharia de dados (ELT) de ponta a ponta. O objetivo aqui Ã© simular um ambiente de produÃ§Ã£o real para a ingestÃ£o, processamento e monitoramento de dados do mercado financeiro (aÃ§Ãµes e notÃ­cias), utilizando uma stack de ferramentas moderno e totalmente orquestrado.

O projeto Ã© 100% "containerizado" usando Docker, permitindo total portabilidade e reprodutibilidade.

ðŸš€ Conceito do Projeto
O MarketPulse captura dois tipos de dados de fontes distintas:

Dados Estruturados: CotaÃ§Ãµes diÃ¡rias de aÃ§Ãµes (OHLCV - Open, High, Low, Close, Volume) da B3 (ex: PETR4) atravÃ©s da API Alpha Vantage.

Dados NÃ£o Estruturados: Manchetes de notÃ­cias do mercado financeiro, coletadas via web scraping do portal InfoMoney e armazenadas em um banco MongoDB.

O objetivo final Ã© mover esses dados brutos atravÃ©s de um Data Lake (AWS S3) e transformÃ¡-los em tabelas limpas e agregadas, prontas para consumo analÃ­tico e de Business Intelligence.

ðŸ›ï¸ Arquitetura e Fluxo de Dados
O pipeline segue a arquitetura Medallion (Bronze, Silver, Gold), orquestrada pelo Apache Airflow.

1. Fontes de Dados (Source)
API de AÃ§Ãµes: Alpha Vantage (Dados estruturados).

MongoDB: Populado por um script de web scraping (BeautifulSoup + requests) que busca notÃ­cias do InfoMoney (Dados nÃ£o estruturados).

2. OrquestraÃ§Ã£o (Apache Airflow)
O coraÃ§Ã£o do projeto, rodando em Docker (com CeleryExecutor, Redis e Postgres). Ele gerencia dois pipelines principais:

Pipeline I (marketpulse_data_ingestion): DAG de ingestÃ£o principal (ELT), agendada diariamente.

extract_stocks_to_bronze (DockerOperator): Uma task que roda uma imagem Docker customizada para buscar dados da API de aÃ§Ãµes e salvÃ¡-los como JSON bruto na Camada Bronze (AWS S3).

extract_news_to_bronze (PythonOperator): Uma task (em paralelo) que lÃª os dados de notÃ­cias do MongoDB (via pymongo) e salva o JSON bruto na Camada Bronze (AWS S3).

# Nova feature
transform_bronze_to_gold (DockerOperator): A etapa de transformaÃ§Ã£o (T). ApÃ³s a ingestÃ£o, esta task dispara um contÃªiner Apache Spark (bitnami/spark) que executa um script PySpark.

Pipeline II (weekly_source_volume_monitoring): DAG de monitoramento e Data Quality, agendada semanalmente.

get_api_volume / get_mongodb_volume (PythonOperators): Tasks que se conectam diretamente Ã s fontes (API e Mongo) para contar o nÃºmero de registros disponÃ­veis na origem.

store_volume_metrics (PostgresHook): Armazena essas contagens em um banco de dados PostgreSQL de metadados, permitindo o acompanhamento da volumetria e detecÃ§Ã£o de anomalias.

3. IngestÃ£o (Camada Bronze - AWS S3)
Os dados brutos (JSONs da API e do Mongo) sÃ£o armazenados no AWS S3 sem modificaÃ§Ã£o, particionados por data e tipo de dado (ex: s3://.../stock_data/ e s3://.../news_data/).

4. TransformaÃ§Ã£o (Spark - Camadas Silver e Gold)
O Objetivo aqui era utilizar os recursos do Databricks para ler os arquivos JSON da camada bronze, transformÃ¡-los e salvÃ¡-los de volta no S3 em camadas otimizadas (Silver e Gold), porÃ©m com mudanÃ§as da versÃ£o Community Edition para o novo Free Tier do databricks, obtive limitaÃ§Ãµes nas permissÃµes para acessar os dados no S3, sendo assim foi aplicado uma soluÃ§Ã£o robusta que Ã© rodar o Spark localmente, dentro do ambiente Docker.

Foi construÃ­do um script PySpark (orquestrado pelo Airflow) para ser responsÃ¡vel por todo o processamento:
Bronze -> Silver: LÃª os JSONs brutos do S3, aplica schemas, limpa (trata nulos, ajusta tipos de dados) e salva os dados em formato colunar otimizado (Parquet) na Camada Silver (S3).

Silver -> Gold: LÃª os dados limpos da Camada Silver e cria tabelas de negÃ³cios, agregadas e prontas para o consumo. Ex: Resumo semanal de aÃ§Ãµes, Contagem de notÃ­cias por dia.

5. MÃ³dulo de DemonstraÃ§Ã£o (Databricks)
Como prova de conceito separada, essa etapa inclui um notebook (.py ou .ipynb) para ser executado no Databricks Free Tier.

Ele demonstra como a mesma lÃ³gica de transformaÃ§Ã£o (Bronze -> Gold) pode ser executada nativamente na plataforma Databricks, lendo dados do DBFS (via upload manual) e salvando-os como Tabelas Delta Lake, que sÃ£o entÃ£o consultadas via Databricks SQL.

ðŸ› ï¸ Tecnologias Utilizadas
OrquestraÃ§Ã£o: Apache Airflow (via Docker Compose)

Bancos de Dados: PostgreSQL (Metastore do Airflow e Metadados do Projeto), MongoDB (Fonte NoSQL)

Processamento de Dados: Apache Spark (PySpark)

Armazenamento (Data Lake): AWS S3

ContÃªineres: Docker & Docker Compose

Bibliotecas Python: pymongo, boto3, requests, beautifulsoup4

Prova de Conceito (Cloud): Databricks (DBFS, Delta Lake, Databricks SQL)

## PrÃ©-requisitos

1.  Conta na AWS (com chaves de Acesso e Secreta)
2.  Um bucket S3 na AWS (anote o nome)
3.  Conta na [Alpha Vantage](https://www.alphavantage.co/) (anote a API Key)
4.  Conta no [Databricks Free Tier](https://databricks.com/try-databricks)
5.  Docker e Docker Compose instalados

## 1. ConfiguraÃ§Ã£o do Ambiente

1.  Clone este repositÃ³rio: `git clone ...`
2.  Navegue atÃ© a pasta de configuraÃ§Ã£o: `cd PROJETOENGDADOS/airflow-environment`
3.  Crie seu arquivo `.env` a partir do exemplo: `cp .env.example .env`
4.  Edite o arquivo `.env` e preencha as senhas `METADATA_DB_PASSWORD` e `MONGO_PASSWORD`.

## 2. Build da Imagem de ExtraÃ§Ã£o

A DAG de ingestÃ£o usa uma imagem Docker customizada. VocÃª precisa "buildar" ela localmente:

1.  Navegue atÃ© a pasta do projeto: `cd PROJETOENGDADOS/marketpulse_project`
2.  Execute o build: `docker build -t marketpulse-extractor:latest .`

## 3. Subindo o Ambiente Airflow

1.  Volte para a pasta do Airflow: `cd ../airflow-environment`
2.  Suba os contÃªineres: `docker-compose up -d --build`
3.  Aguarde alguns minutos e acesse o Airflow em `http://localhost:8080` (usuÃ¡rio/senha padrÃ£o: `airflow`/`airflow`).

## 4. ConfiguraÃ§Ã£o PÃ³s-Subida (Airflow e Databricks)

VocÃª precisa configurar o Airflow e o Databricks manualmente:

### No Airflow (localhost:8080):

1.  **VariÃ¡veis:** VÃ¡ em `Admin -> Variables` e crie:
    * `aws_access_key_id`: (Sua chave de acesso AWS)
    * `aws_secret_access_key`: (Sua chave secreta AWS)
    * `alpha_vantage_api_key`: (Sua chave da Alpha Vantage)
2.  **ConexÃµes:** VÃ¡ em `Admin -> Connections -> +` e crie:
    * **Conn Id:** `mongo_marketpulse_db`
    * **Conn Type:** `Generic`
    * **Host:** `mongo`
    * **Login:** `mongoadmin`
    * **Password:** (A senha que vocÃª definiu no `.env` para `MONGO_PASSWORD`)
    * **Port:** `27017`
    * **Extra:** `{"database": "marketpulse_news"}`

### No Databricks:

1.  Crie um novo Notebook (ex: `NotebookMarketPulse`).
2.  Copie o cÃ³digo do arquivo `databricks_notebook.py` (vocÃª deve criar este arquivo no seu projeto) e cole no notebook.
3.  Execute a cÃ©lula de criaÃ§Ã£o dos Widgets.
4.  Execute a cÃ©lula de teste de leitura, preenchendo os Widgets com suas chaves AWS.

## 5. ExecuÃ§Ã£o

1.  No Airflow, ative (unpause) as DAGs `marketpulse_data_ingestion` e `weekly_source_volume_monitoring`.
2.  Dispare a `marketpulse_data_ingestion` manualmente para popular a Camada Bronze.
3.  Execute o notebook Databricks para processar os dados (Bronze -> Silver -> Gold).
